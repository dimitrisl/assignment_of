{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the following dataset (ASSIGNMENT.csv), you can pick either the recording or the composition data included and work on the\n",
    "following format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| title | writers |\n",
    "| --- | --- |\n",
    "| Yellow submarine | Leo Ouha |\n",
    "| Anaconda | Mick George |\n",
    "| Shape of you | Ed Sheeran |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extract the top 100 keywords in the title using TfidfVectorizer.\n",
    "\n",
    "* Remove stopwords and calculate the same.\n",
    "\n",
    "* Extract the top 100 2- grams and 3-grams (user term as gram not characters)\n",
    "\n",
    "* Extract the list of unique writers and calculate their frequency in the dataset.\n",
    "\n",
    "* Calculate the top 10 co-occurrence of writers .\n",
    "\n",
    "* Recognize the duplicates in the dataset and export a csv with the fixed rows.\n",
    "\n",
    "* Report and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ASSIGNMENT.csv\"\n",
    "dataset = pd.read_csv(\"dataset/%s\"%filename)\n",
    "composition_cols = [i for i in dataset.columns if \"comp\" in i.lower()]\n",
    "recording_cols = [i for i in dataset.columns if \"recording\" in i.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                              Composition Title  \\\n0                                       KOKAINA   \n1                             POR ESTAR CONTIGO   \n2         Gallardo (feat. Rick Ross & Yo Gotti)   \n3                             LESSON IN LEAVING   \n4  QUÃ©DATE EN MIS BRAZOS QUEDATE EN MIS BRAZOS   \n\n                                 Composition Writers  \\\n0                       YASSINE BAYBAH|DANIEL DLOUHY   \n1                 MARTINEZ ESCAMILLA,FELIPE DE JESUS   \n2  William Alfred / Karmin Kharbouch / Mario Mims...   \n3                                  MAHER B/GOODRUM C   \n4                                     KIKE SANTANDER   \n\n                                   Recording Title  \\\n0                                          Kokaina   \n1                                    Estar Contigo   \n2  Connect the Dots (feat. Yo Gotti and Rick Ross)   \n3                                Lesson In Leavin'   \n4                            Quédate En Mis Brazos   \n\n                                   Recording Writers  \n0                                  A BAYBAH C DLOUHY  \n1             MARTINEZ DE UBAGO RODRIGUEZ  ALEJANDRO  \n2  MARIO MIMS|NIKOLAS PAPAMITROU|RICK ROSS|ROBERT...  \n3                                            GOODRUM  \n4                                    SANTANDER  KIKE  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Composition Title</th>\n      <th>Composition Writers</th>\n      <th>Recording Title</th>\n      <th>Recording Writers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KOKAINA</td>\n      <td>YASSINE BAYBAH|DANIEL DLOUHY</td>\n      <td>Kokaina</td>\n      <td>A BAYBAH C DLOUHY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>POR ESTAR CONTIGO</td>\n      <td>MARTINEZ ESCAMILLA,FELIPE DE JESUS</td>\n      <td>Estar Contigo</td>\n      <td>MARTINEZ DE UBAGO RODRIGUEZ  ALEJANDRO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Gallardo (feat. Rick Ross &amp; Yo Gotti)</td>\n      <td>William Alfred / Karmin Kharbouch / Mario Mims...</td>\n      <td>Connect the Dots (feat. Yo Gotti and Rick Ross)</td>\n      <td>MARIO MIMS|NIKOLAS PAPAMITROU|RICK ROSS|ROBERT...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LESSON IN LEAVING</td>\n      <td>MAHER B/GOODRUM C</td>\n      <td>Lesson In Leavin'</td>\n      <td>GOODRUM</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>QUÃ©DATE EN MIS BRAZOS QUEDATE EN MIS BRAZOS</td>\n      <td>KIKE SANTANDER</td>\n      <td>Quédate En Mis Brazos</td>\n      <td>SANTANDER  KIKE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dataset = dataset.loc[:, composition_cols].copy()\n",
    "record_dataset = dataset.loc[:, recording_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Composition Title      False\nComposition Writers     True\nRecording Title        False\nRecording Writers       True\ndtype: bool"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if there are any nan values in the dataset.\n",
    "\n",
    "dataset.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            Composition Title Composition Writers  \\\n494               THIS IS WAR                 NaN   \n749              BYE BYE BABY                 NaN   \n1031              Think About                 NaN   \n1115  WHERE DO I GO FROM HERE                 NaN   \n1419                WALK AWAY                 NaN   \n\n                                        Recording Title  \\\n494   Falconshield - This Is War 2: Piltover vs Zaun...   \n749                                        BYE BYE BABY   \n1031                        뮤직비디오 1번트랙 Think About  Chu   \n1115                            WHERE DO I GO FROM HERE   \n1419        Dokken - \"Walk Away\" (Official Music Video)   \n\n                                      Recording Writers  \n494     CA FLOBERG MARTIN DAVID/PA FALCONSHIELD/obo NCB  \n749                                BOB CREWE/BOB GAUDIO  \n1031  ASOTOUNION 1/ASOTOUNION 2/ASOTOUNION 3/ASOTOUN...  \n1115                        LARRY GROSSMAN;MARTY PANZER  \n1419             DOKKEN, DON/LYNCH, GEORGE/PILSON, JEFF  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Composition Title</th>\n      <th>Composition Writers</th>\n      <th>Recording Title</th>\n      <th>Recording Writers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>494</th>\n      <td>THIS IS WAR</td>\n      <td>NaN</td>\n      <td>Falconshield - This Is War 2: Piltover vs Zaun...</td>\n      <td>CA FLOBERG MARTIN DAVID/PA FALCONSHIELD/obo NCB</td>\n    </tr>\n    <tr>\n      <th>749</th>\n      <td>BYE BYE BABY</td>\n      <td>NaN</td>\n      <td>BYE BYE BABY</td>\n      <td>BOB CREWE/BOB GAUDIO</td>\n    </tr>\n    <tr>\n      <th>1031</th>\n      <td>Think About</td>\n      <td>NaN</td>\n      <td>뮤직비디오 1번트랙 Think About  Chu</td>\n      <td>ASOTOUNION 1/ASOTOUNION 2/ASOTOUNION 3/ASOTOUN...</td>\n    </tr>\n    <tr>\n      <th>1115</th>\n      <td>WHERE DO I GO FROM HERE</td>\n      <td>NaN</td>\n      <td>WHERE DO I GO FROM HERE</td>\n      <td>LARRY GROSSMAN;MARTY PANZER</td>\n    </tr>\n    <tr>\n      <th>1419</th>\n      <td>WALK AWAY</td>\n      <td>NaN</td>\n      <td>Dokken - \"Walk Away\" (Official Music Video)</td>\n      <td>DOKKEN, DON/LYNCH, GEORGE/PILSON, JEFF</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset[\"Composition Writers\"].isna(), :].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                      Composition Title            Composition Writers  \\\n17                    KING OF THE NIGHT            Thomas Sean MCMAHON   \n31              GIVE ME ONE MORE CHANCE        ABRAHAM JR. QUINTANILLA   \n35                    SI QUIERES AMARLA    DOURGE,PAUL|VILAS,GUILLERMO   \n39  REBECCA & JACK (THAT'LL BE THE DAY)             KHOSLA, SIDDHARTHA   \n49                    LETTRE A MA SOEUR  BRAMS|KAMELANCIEN|MARIA,CHEBA   \n\n                              Recording Title Recording Writers  \n17                          KING OF THE NIGHT               NaN  \n31                    Give Me One More Chance               NaN  \n35                          Si Quieres Amarla               NaN  \n39  REBECCA & JACK (THAT'LL BE THE DAY)-28221               NaN  \n49      Lettre à ma soeur (feat. Cheba Maria)               NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Composition Title</th>\n      <th>Composition Writers</th>\n      <th>Recording Title</th>\n      <th>Recording Writers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>KING OF THE NIGHT</td>\n      <td>Thomas Sean MCMAHON</td>\n      <td>KING OF THE NIGHT</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>GIVE ME ONE MORE CHANCE</td>\n      <td>ABRAHAM JR. QUINTANILLA</td>\n      <td>Give Me One More Chance</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>SI QUIERES AMARLA</td>\n      <td>DOURGE,PAUL|VILAS,GUILLERMO</td>\n      <td>Si Quieres Amarla</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>REBECCA &amp; JACK (THAT'LL BE THE DAY)</td>\n      <td>KHOSLA, SIDDHARTHA</td>\n      <td>REBECCA &amp; JACK (THAT'LL BE THE DAY)-28221</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>LETTRE A MA SOEUR</td>\n      <td>BRAMS|KAMELANCIEN|MARIA,CHEBA</td>\n      <td>Lettre à ma soeur (feat. Cheba Maria)</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset[\"Recording Writers\"].isna(), :].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the first run i will pick the composisition title & writers pair ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all I'm going to remove those nan values that exist in the dataset since they dont offer any information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dataset = comp_dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards we will lowercase every token / sentence in order to have a uniformity in our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(comp_dataset.shape[-1]):\n",
    "    comp_dataset.iloc[:, i] = comp_dataset.iloc[:,i].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dataset.columns = [\"Title\", \"Writers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_values(comp_dataset, n=100, stopwords=False):\n",
    "    if stopwords:\n",
    "        stopwords = \"english\"\n",
    "        tfidf_vect = TfidfVectorizer(analyzer='word', stop_words=stopwords)\n",
    "    else:\n",
    "        tfidf_vect = TfidfVectorizer(analyzer='word')\n",
    "        \n",
    "    tfidf_wm = tfidf_vect.fit_transform([\" \".join(comp_dataset[\"Title\"].tolist())])\n",
    "    tfidf_tokens = tfidf_vect.get_feature_names()\n",
    "    results = pd.DataFrame()\n",
    "    results[\"score\"] = tfidf_wm.data\n",
    "    results.index = tfidf_tokens\n",
    "    results = results.sort_values(by=\"score\").tail(n)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_grams(comp_dataset,n_gram, field, n=100):\n",
    "    corpora = \" \".join(comp_dataset[field].tolist())\n",
    "    ngram_counts = Counter(ngrams(corpora.split(), n_gram))\n",
    "    return ngram_counts.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlolis/Desktop/assignment/assignment_of/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "             score\nsilent    0.020404\nversion   0.020404\nvu        0.020404\nsamba     0.022259\nsoldiers  0.022259\n...            ...\nwrld      0.194762\nyadan     0.252264\nwhomp     0.315329\nwow       0.411783\nye        0.422912\n\n[100 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>silent</th>\n      <td>0.020404</td>\n    </tr>\n    <tr>\n      <th>version</th>\n      <td>0.020404</td>\n    </tr>\n    <tr>\n      <th>vu</th>\n      <td>0.020404</td>\n    </tr>\n    <tr>\n      <th>samba</th>\n      <td>0.022259</td>\n    </tr>\n    <tr>\n      <th>soldiers</th>\n      <td>0.022259</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>wrld</th>\n      <td>0.194762</td>\n    </tr>\n    <tr>\n      <th>yadan</th>\n      <td>0.252264</td>\n    </tr>\n    <tr>\n      <th>whomp</th>\n      <td>0.315329</td>\n    </tr>\n    <tr>\n      <th>wow</th>\n      <td>0.411783</td>\n    </tr>\n    <tr>\n      <th>ye</th>\n      <td>0.422912</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_values(comp_dataset.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlolis/Desktop/assignment/assignment_of/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "             score\nkorsakov  0.027668\npotro     0.027668\nwallace   0.027668\nupside    0.027668\nvogue     0.027668\n...            ...\nwolves    0.154151\nturn      0.166009\nwavin     0.185772\nvichre    0.312255\nyankee    0.537554\n\n[100 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>korsakov</th>\n      <td>0.027668</td>\n    </tr>\n    <tr>\n      <th>potro</th>\n      <td>0.027668</td>\n    </tr>\n    <tr>\n      <th>wallace</th>\n      <td>0.027668</td>\n    </tr>\n    <tr>\n      <th>upside</th>\n      <td>0.027668</td>\n    </tr>\n    <tr>\n      <th>vogue</th>\n      <td>0.027668</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>wolves</th>\n      <td>0.154151</td>\n    </tr>\n    <tr>\n      <th>turn</th>\n      <td>0.166009</td>\n    </tr>\n    <tr>\n      <th>wavin</th>\n      <td>0.185772</td>\n    </tr>\n    <tr>\n      <th>vichre</th>\n      <td>0.312255</td>\n    </tr>\n    <tr>\n      <th>yankee</th>\n      <td>0.537554</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_values(comp_dataset.copy(), stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the top 100 2-grams and 3-grams (user term as gram not characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'the'), 30), (('love', 'you'), 29), (('i', 'love'), 24), (('in', 'the'), 22), (('the', 'world'), 16), (('i', 'am'), 16), (('it', 'is'), 14), (('to', 'me'), 13), (('bad', 'asset'), 12), (('let', 'me'), 12), (('flower', 'of'), 11), (('of', 'scotland'), 11), (('no', 'te'), 11), (('with', 'me'), 10), (('to', 'the'), 10), (('i', 'want'), 10), (('want', 'to'), 10), (('love', 'me'), 10), (('on', 'you'), 10), (('my', 'life'), 9), (('do', 'not'), 9), (('for', 'you'), 9), (('baby', 'i'), 9), (('on', 'my'), 9), (('my', 'name'), 9), (('i', 'got'), 9), (('on', 'the'), 8), (('we', 'have'), 8), (('you', 'are'), 8), (('te', 'quiero'), 8), (('you', 'love'), 8), (('are', 'you'), 8), (('do', 'it'), 8), (('por', 'ti'), 8), (('with', 'you'), 8), (('que', 'no'), 8), (('amazing', 'grace'), 8), (('in', 'a'), 8), (('you', 'go'), 7), (('more', 'than'), 7), (('of', 'a'), 7), (('do', 'you'), 7), (('this', 'is'), 7), (('i', 'need'), 7), (('all', 'the'), 7), (('hold', 'on'), 7), (('my', 'eyes'), 6), (('i', 'believe'), 6), (('give', 'me'), 6), (('un', 'millon'), 6), (('millon', 'de'), 6), (('love', 'is'), 6), (('white', 'christmas'), 6), (('all', 'night'), 6), (('if', 'you'), 6), (('de', 'amor'), 6), (('out', 'my'), 6), (('a', 'g'), 6), (('in', 'love'), 6), (('the', 'one'), 6), (('day', 'in'), 6), (('me', 'no'), 6), (('no', 'me'), 6), (('you', 'and'), 6), (('this', 'christmas'), 6), (('and', 'the'), 6), (('be', 'with'), 5), (('little', 'star'), 5), (('let', 'you'), 5), (('than', 'feeling'), 5), (('take', 'me'), 5), (('i', 'do'), 5), (('a', 'friend'), 5), (('friend', 'we'), 5), (('have', 'in'), 5), (('in', 'jesus'), 5), (('we', 'are'), 5), (('my', 'mind'), 5), ((\"i'll\", 'be'), 5), (('it', 'on'), 5), (('me', 'i'), 5), (('get', 'it'), 5), (('call', 'out'), 5), (('me', 'a'), 5), (('love', '(remix)'), 5), (('you', 'know'), 5), (('te', 'vayas'), 5), (('me', 'love'), 5), (('your', 'love'), 5), (('want', 'you'), 5), (('i', \"don't\"), 5), (('one', 'that'), 5), (('that', 'got'), 5), (('got', 'away'), 5), (('christmas', '(hang'), 5), (('(hang', 'all'), 5), (('the', 'mistletoe)'), 5), (('o', 'come,'), 5), (('o', 'come'), 5), (('in', 'my'), 5)]\n"
     ]
    }
   ],
   "source": [
    "print(get_top_n_grams(comp_dataset, 2,\"Title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('i', 'love', 'you'), 20), (('flower', 'of', 'scotland'), 11), (('un', 'millon', 'de'), 6), (('you', 'love', 'me'), 6), (('of', 'the', 'world'), 6), (('baby', 'i', 'love'), 6), (('more', 'than', 'feeling'), 5), (('friend', 'we', 'have'), 5), (('we', 'have', 'in'), 5), (('have', 'in', 'jesus'), 5), (('call', 'out', 'my'), 5), (('out', 'my', 'name'), 5), (('no', 'te', 'vayas'), 5), (('let', 'me', 'love'), 5), (('one', 'that', 'got'), 5), (('that', 'got', 'away'), 5), (('this', 'christmas', '(hang'), 5), (('christmas', '(hang', 'all'), 5), (('(hang', 'all', 'the'), 5), (('all', 'the', 'mistletoe)'), 5), (('twinkle', 'twinkle', 'little'), 4), (('twinkle', 'little', 'star'), 4), (('millon', 'de', 'lagrimas'), 4), (('let', 'you', 'go'), 4), (('what', 'a', 'friend'), 4), (('a', 'friend', 'we'), 4), (('lovely', 'day', '(part'), 4), (('day', '(part', 'ii)'), 4), (('i', 'want', 'to'), 4), (('fields', 'of', 'athenry'), 4), (('i', 'want', 'you'), 4), (('be', 'with', 'you'), 4), (('got', 'my', 'eyes'), 4), (('my', 'eyes', 'on'), 4), (('eyes', 'on', 'you'), 4), (('girls', 'just', 'want'), 4), (('just', 'want', 'to'), 4), (('want', 'to', 'have'), 4), (('to', 'have', 'fun'), 4), (('all', 'about', 'me'), 4), (('of', 'my', 'life'), 4), (('stay', 'with', 'me'), 4), (('into', 'my', 'life'), 3), (('there', 'for', 'you'), 3), (('on', 'my', 'mind'), 3), (('take', 'me', 'away'), 3), (('o', 'sole', 'mio'), 3), (('are', 'you', 'ready'), 3), (('rudolf', 'the', 'red'), 3), (('the', 'red', 'nosed'), 3), (('red', 'nosed', 'reindeer'), 3), (('hay', 'algo', 'en'), 3), (('algo', 'en', 'ti'), 3), (('en', 'ti', '2'), 3), (('bury', 'me', 'a'), 3), (('me', 'a', 'g'), 3), (('qian', 'li', 'zhi'), 3), (('li', 'zhi', 'wai'), 3), ((\"i'm\", 'a', 'g'), 3), (('joy', 'to', 'the'), 3), (('to', 'the', 'world'), 3), (('loco', 'por', 'ti'), 3), (('god', 'rest', 'ye'), 3), (('lost', 'in', 'japan'), 3), (('another', 'day', 'in'), 3), (('day', 'in', 'paradise'), 3), (('me', 'no', 'me'), 3), (('no', 'me', 'digas'), 3), (('me', 'digas', 'que'), 3), (('digas', 'que', 'no'), 3), (('the', 'end', 'of'), 3), (('end', 'of', 'the'), 3), (('me', 'love', 'you'), 3), (('angels', 'we', 'have'), 3), (('we', 'have', 'heard'), 3), (('have', 'heard', 'on'), 3), (('say', 'my', 'name'), 3), (('in', 'the', 'dark'), 3), (('welcome', 'to', 'my'), 3), (('havana', '(feat.', 'daddy'), 3), (('(feat.', 'daddy', 'yankee)'), 3), (('o', 'come,', 'o'), 3), (('come,', 'o', 'come'), 3), (('o', 'come', 'emmanuel'), 3), (('let', 'us', 'go'), 3), (('i', 'come', 'from'), 3), (('yo', 'que', 'no'), 3), (('que', 'no', 'vivo'), 3), (('no', 'vivo', 'sin'), 3), (('vivo', 'sin', 'ti'), 3), (('break', 'up', 'with'), 3), (('up', 'with', 'your'), 3), (('with', 'your', 'girlfriend'), 3), (('row', 'your', 'boat'), 3), (('all', 'night', 'long'), 3), (('this', 'is', 'my'), 3), ((\"i've\", 'got', 'my'), 3), (('you', 'and', 'i'), 3), (('miss', \"kissin'\", 'on'), 3), ((\"kissin'\", 'on', 'you'), 3)]\n"
     ]
    }
   ],
   "source": [
    "print(get_top_n_grams(comp_dataset, 3,\"Title\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the list of unique writers and calculate their frequency in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The writers are **not seperated** in the same way all in all the rows. Given that, I will have to do a best effort\n",
    "to extract the unique writers out of the dataset. It is not possible to use NER here because we don't have free text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "seperators = [\"|\", \"/\", \"\\\\\",\"+\",\"-\", \"\\n\", \"\\t\", \",\",\";\"]\n",
    "# these are the most common separators that were observed in the dataset.\n",
    "writers = comp_dataset[\"Writers\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers_with_seperators = [i for i in writers if len(set(seperators).intersection(set(i)))]\n",
    "writers_without_seperators = [i for i in writers if not len(set(seperators).intersection(set(i)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writers with seperators :  1536\n"
     ]
    }
   ],
   "source": [
    "print(\"writers with seperators : \", len(writers_with_seperators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writers without seperators:  1020\n"
     ]
    }
   ],
   "source": [
    "print(\"writers without seperators: \", len(writers_without_seperators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we will try to get order out of the writers that contain seperators assuming that they have a uniformity in their seperation e.g. \"artist A, artist B\" and not \"artist A | artist B , artist C\".\n",
    "\n",
    "After getting those arists seperated i will try to get the 2-grams out of the artists without a seperator.\n",
    "The final (best effort) unique arstist will be the union of those two sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_g_a = []\n",
    "for i in writers_with_seperators:\n",
    "    common_sep = list(set(seperators).intersection(i))\n",
    "    tmp = i.split(common_sep[0])\n",
    "    tmp = [x.strip() for x in tmp]\n",
    "    artists_g_a.extend(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['yassine baybah',\n 'daniel dlouhy',\n 'martinez escamilla',\n 'felipe de jesus',\n 'william alfred',\n 'karmin kharbouch',\n 'mario mims',\n 'richard morales',\n 'rick ross',\n 'maher b']"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists_g_a[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that each artist has 2 names e.g. name and surname in this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_artists = []\n",
    "wwsep = []\n",
    "for i in writers_without_seperators:\n",
    "    if len(i.split(\" \")) == 2:\n",
    "        unique_artists.append(i)\n",
    "    else:\n",
    "        wwsep.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "615"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wwsep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = \" \".join(wwsep)\n",
    "ngram_counts = Counter(ngrams(corpora.split(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Assume that the most common 100 contain 100 unique artists out of the list of 615 that remained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_list = []\n",
    "for x,y in ngram_counts.most_common(100):\n",
    "    last_list.append(\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_artists = last_list + unique_artists + artists_g_a\n",
    "all_artists = [i.strip() for i in all_artists if len(i.strip())>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique artists are 4377\n"
     ]
    }
   ],
   "source": [
    "print(\"unique artists are\", len(list(set(last_list + unique_artists + artists_g_a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_occurences = Counter(last_list + unique_artists + artists_g_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('traditional', 20),\n ('trad', 12),\n ('sean combs', 11),\n ('nusrat fateh ali khan', 11),\n ('dp', 11),\n ('john', 10),\n ('pharrell williams', 7),\n ('louis bell', 7),\n ('williams', 7),\n ('aubrey graham', 6)]"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_occurences.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_keys = list(artist_occurences.keys())\n",
    "for i in fixed_keys:\n",
    "    if len(i)<3:\n",
    "        artist_occurences.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = []\n",
    "\n",
    "for line in comp_dataset[\"Writers\"].tolist():\n",
    "    line_l = set()\n",
    "    for artist in artist_occurences:\n",
    "        if artist in line:\n",
    "            line_l.add(artist)\n",
    "    if len(line_l):\n",
    "        keep.append(line_l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_intersections = dict()\n",
    "\n",
    "for idi, i in enumerate(keep):\n",
    "    for idj, j in enumerate(keep):\n",
    "        if idj != idi:\n",
    "            intersect = i.intersection(j)\n",
    "            if len(intersect)<2:\n",
    "                continue\n",
    "            intersect = str(sorted(list(intersect)))\n",
    "            # intersect = str(list(intersect).sort())\n",
    "            if intersect in dict_intersections:\n",
    "                dict_intersections[intersect] +=1\n",
    "            else:\n",
    "                dict_intersections[intersect] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the top 10 co-occurrence of writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_artists = sorted([(i[-1], i[0]) for i in dict_intersections.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "[(3573, \"['chris', 'christopher']\"),\n (1973, \"['trad', 'traditional']\"),\n (1627, \"['jose', 'joseph']\"),\n (1363, \"['dan', 'daniel']\"),\n (1239, \"['carlo', 'carlos']\"),\n (1015, \"['jack', 'jackson']\"),\n (797, \"['john', 'johnson']\"),\n (249, \"['bob', 'bobby']\"),\n (241, \"['jose', 'jose luis']\"),\n (173, \"['live', 'oliver']\")]"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_artists[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate from the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop_duplicates()\n",
    "dataset.to_csv(\"excersize_1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}